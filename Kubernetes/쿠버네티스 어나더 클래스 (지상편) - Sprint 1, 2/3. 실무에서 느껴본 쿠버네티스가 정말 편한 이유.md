# 0. 인트로
## 쿠버네티스 엔지니어로 전향한 이유 

> **한 우물의 조건**
> 1. 재미가 있어야함
> 2. 더 쉽게 적용
> 3. 많은 도메인에서 쓰는 기술 

- si : 제안서, 업무프로세스 분석, 단계별 문서, 인프라 구축, full stack 
- 고객사마다 다르다 : 매번 다른 업무 프로세스, 변화가 빠른 웹 기술, 다양한 기술 솔루션 

- 가상화 플랫폼 개발 : 이미 대기업은 구축되는 상황이 많음 
	- Openstack + Infra
	- 쿠버네티스 

- 쿠버네티스 엔지니어 역할 <- 새로운 기술 먼저 시작하자!


## 쿠버네티스 표준 생태계로 편해진 IT 인프라 구축

- CNCF 프로젝트 단계 : Graduated > Incubating > Sandbox > Archived
- 깃허브 star 수 고려 

- 츨처 : 쿠버네티스 어나더 클래스 Sprint 1, 2 섹션 5 
![[Pasted image 20250529074626.png]] 


## 0) 실제 프로젝트를 할 때 구조적인 문제 (모니터링/로깅)

1. 개발과 모니터링 시스템이 서로 엮일 수 밖에 없는 구조 
	모니터링/로깅을 하려면 개발 패키지에 에이전트를 심거나 개발 코드 변경
	-> 성능 테스트 때 개발 기능이 안나오면 에이전트를 의심하기도 함
	
2. 개발에서는 한번도 써보지 않은 (개발 시스템을 위한) 모니터링 시스템을 만드는 구조 
	 모니터링 시스템은 개발 초반부터 개발자들이 써보면서 불편한 점이나 주요 모니터링 포인트를 수용하면서 만들어지는 게 이상적이지만, 프로젝트가 같이 시작되니까 처음엔 쓸 수가 없다 
	 -> 로우 레벨로 로그나 성능을 찾아보면서 장애를 분석하게 된다 
	 -> 나중에 모니터링 시스템이 오픈되어도 익숙한 방법이 있으니까 안 보게 된다 

3. 초기 개발 계획과 다르게 요구사항이 변경되면서 APP 변경이 있을 수 있음
	 모니터링 시스템에 APP 자동 감지 시스템이 없을 수도 있음
	 모니터링 시스템 개발자가 사라질 수도?
	 -> 오픈시 개발 프로젝트와 서로 다른 범위의 APP들을 모니터링하게 되는 구조 

> 위의 문제들을 해결할 수 있다 


# 1. 모니터링 설치 : Prometheus, Grafana, Loki-Stack
> 링크 : https://cafe.naver.com/kubeops/30

---
## 1) Git에서 sparse-checkout으로 필요한 거 받기 

```bash
[root@k8s-master ~]# 

yum -y install git

# 로컬 저장소 생성 
git init monitoring 
git config --global init.defaultBranch main #컴퓨터의 defaultBranch를 main으로 설정하기
cd monitoring

git remote add -f origin https://github.com/k8s-1pro/install.git

# -----------------------------------------------------

# 전체가 아니라 일부 디렉토리만 선택적으로 다운로드하기 위한 기능 

# 1) git 버전 2.25 이상 >>>  
git sparse-checkout init --cone
git sparse-checkout set ground/k8s-1.27/prometheus-2.44.0 ground/k8s-1.27/loki-stack-2.6.1

# 2) git 버전 2.24 이하 >>>
# git config core.sparseCheckout true 
# echo "ground/k8s-1.27/prometheus-2.44.0" >> .git/info/sparse-checkout 
# echo "ground/k8s-1.27/loki-stack-2.6.1" >> .git/info/sparse-checkout

git pull origin main 
```
![[Pasted image 20250529085531.png]]
![[Pasted image 20250529085543.png]] 



## 2) Prometheus (with Grafana) 설치

```bash

#설치 ([root@k8s-master monitoring]#) 
kubectl apply --server-side -f ground/k8s-1.27/prometheus-2.44.0/manifests/setup 

kubectl wait --for condition=Established --all CustomResourceDefinition

kubectl apply -f ground/k8s-1.27/prometheus-2.44.0/manifests 

# 설치 확인 ([root@k8s-master]#) => 오래 걸림 

kubectl get pods -n monitoring 

```

- `kubectl apply` : Kubernetes 리소스 생성/수정 명령 (선언적 방식)
- `--server-side` : 리소스를 **클라이언트가 아닌 API 서버에서 병합**하여 적용하겠다는 뜻
- `-f <경로>` : 적용할 리소스 YAML 파일 또는 디렉터리 지정


- `--server-side`와 `--client-side` 옵션의 차이점 
	- `--server-side`
		- 병합위치 : API 서버가 직접 병합함 
		- 성능 : 서버에서 빠르게 처리
		- 충돌 해결 : 서버가 변경 사항 병합
		- 주로 사용 : 컨트롤러나 CRD 같은 리소스 적용, GitOps 도구, 팀 협업
	- `--client-side`
		- 병합위치 : 로컬에서 머지 후 API 서버에 보냄
		- 성능 : 리소스가 크면 느릴 수 있음
		- 충돌 해결 : 로컬에서 해결 필요
		- 주로 사용 : 간단한 적용, 실습 


- `kubectl wait --for condition=Established --all CustomResourceDefinition`
	- Prometheus Operator, Argo CD, Istio 등에서 CRD를 먼저 apply한 뒤, 해당 CRD가 제대로 등록될때까지 기다리는 용도로 사용 

## 3) Loki-Stack 설치 
```bash
# 설치 ([root@k8s-master monitoring]#) 
kubectl apply -f ground/k8s-1.27/loki-stack-2.6.1 

# 설치 확인 
kubectl get pods -n loki-stack
```

- `-n` : namespace 지정 명령어

## 4) 상태 확인
> 되게 오래 걸렸다 
![[Pasted image 20250529095105.png]]

## 4-1) 그라파나 접속
-  http://192.168.56.30:30001
- id: admin, pw: admin
![[Pasted image 20250529102342.png]]

## +) Prometheus(with Grafana), Loki-stack 삭제 명령어 
```bash
[root@k8s-master ~]# 

cd monitoring 
# Prometheus 삭제 
kubectl delete --ignore-not-found=true -f ground/k8s-1.27/prometheus-2.44.0/manifests -f ground/k8s-1.27/prometheus-2.44.0/manifests/setup 
# Loki-stack 삭제 
kubectl delete -f ground/k8s-1.27/loki-stack-2.6.1
```


# 2. 쿠버네티스 대표 기능 - Traffic Routing, Self-Healing, AutoScaling, RollingUpdate 
> 링크 : https://cafe.naver.com/kubeops/31


## 1) APP 배포 

> dashboard : https://192.168.56.30:30000/#/login

### (1) default 네임스페이스에 리소스 생성 

#### 대시보드 활용하여 생성 
- namespace : default, +버튼 클릭하기 
![[Pasted image 20250529092726.png]]

##### 현재 선택된 네임스페이스에 생성할 리소스를 명시하는 YAML 

```yaml
apiVersion: apps/v1
kind: Deployment  #(1) Deployment : 지정한 수만큼의 Pod를 유지 + 버전 관리/롤링 업데이트 등도 자동으로 처리
metadata:
  name: app-1-2-2-1
spec:
  selector:
    matchLabels:  # (a) 어떤 Pod를 대상으로 할지 지정 
      app: '1.2.2.1'
  replicas: 2   # (1-1) Pod 이중화 
  strategy:
    type: RollingUpdate  # 무중단 배포 방식 중 하나 
  template:
    metadata:
      labels:
        app: '1.2.2.1'  # (a-1) 실제 Pod가 붙게될 라벨로 위 selector와 일치해야 함 
    spec:
      containers:  # (b) 컨테이너 설정 
        - name: app-1-2-2-1
          image: 1pro/app
          imagePullPolicy: Always 
          ports:
            - name: http
              containerPort: 8080
          startupProbe: # (b-1) 컨테이너가 시작 준비 중일 때 상태 체크 
            httpGet:
              path: "/ready"
              port: http
            failureThreshold: 20  # 20회 실패까지 ㄱㅊ
          livenessProbe:  # (b-1-1) 컨테이너가 죽었는지 체크 (실패시 재시작)
            httpGet:
              path: "/ready"
              port: http
          readinessProbe: # (b-1-2) 트래픽 받을 준비되었는지 확인(안되면 서비스에서 제외)
            httpGet:
              path: "/ready"
              port: http
          resources:  # (b-2) 리소스 제한 
            requests: #(b-2-1) 최소 할당 (스케일링 기준)
              memory: "100Mi"
              cpu: "100m"
            limits:  #(b-2-2) 최대 사용 허용량 (초과시 throttling)
              memory: "200Mi"
              cpu: "200m"
---
apiVersion: v1
kind: Service  # (2) Service : pod들에게 트래픽 분산 
metadata:
  name: app-1-2-2-1
spec:
  type: NodePort
  selector:
    app: '1.2.2.1'  # (2-2) 라벨이 다음과 같은 Pod들과 연결됨 
  ports:
    - port: 8080  # (c) 외부 요청 port 
      targetPort: 8080  # (c-1) 내부 컨테이너의 이 포트로 전달됨 
      nodePort: 31221  #(2-1) Port 지정 : 마스터 노드에서 이 port로 트래픽 보냈을때
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler   # (3) 스케일링 설정 : 부하에 따라 Pod 수 자동 조절 
metadata:
  name: app-1-2-2-1
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-1-2-2-1
  minReplicas: 2     # (3-1) 최소 2개에서 
  maxReplicas: 4     # (3-2) 최대 4개까지 늘어날 수 있음
  metrics:
    - type: Resource
      resource:
        name: cpu    # (3-3) CPU 기준 
        target:
          type: Utilization
          averageUtilization: 40  # (3-4) 평균 40% 넘어야 늘어남 
```

#### Kubernetes Dashboard에서 확인해보자

- 파드 2개 생성 확인
![[Pasted image 20250529105403.png]]
- 서비스 설정 확인

![[Pasted image 20250529105349.png]]

- 그라파나에서도 확인 가능하다 !

![[Pasted image 20250529102445.png]]


## 2) 쿠버네티스 여러 기능 테스트
### (1) APP에 지속적으로 트래픽 보내기 (Traffic Routing 테스트 )

```bash
[root@k8s-master ~]# 
while true; do curl http://192.168.56.30:31221/hostname; sleep 2; echo ''; done;
```

![[Pasted image 20250529105516.png]]
2개의 Pod에 골고루 보내지는 것을 알 수 있다 

![[3.0. 발생했던 문제 - 31221 connection refused]]



### (2) App에 Memory Leak 나게 하기 (self-Healing 테스트)
> app이 죽자마자 재시작 
```bash
[root@k8s-master ~]# 
curl 192.168.56.30:31221/memory-leak 
```

![[3.1. 장애 원인 찾는 로직을 따라해보자]]


##### 그외 : 설정대로 Pod개수가 최대 4개까지 늘어났음을 볼 수 있다 

![[Pasted image 20250529105858.png]]




### (3) App에 부하주기 (AutoScaling 테스트)
```bash
[root@k8s-master ~]# 
curl 192.168.56.30:31221/cpu-load
```

![[Pasted image 20250529113854.png]]

- 새로 하나 생성되고 있는 과정이 보인다 -> 총 4개까지 생성됨 (2개 추가 생성성)
![[Pasted image 20250529113835.png]]



### (4) (RollingUpdate 테스트)

#### (4-1) App 이미지 업데이트 
> namespace:default > 디플로이먼트 > ... > 편집

```yaml
spec:
      containers:
        - name: app-1-2-2-1
          image: 1pro/app-update  # 수정
```

> kubectl 명령으로 할 경우 

```bash
[root@k8s-master ~]# 

kubectl set image -n default deployment/app-1-2-2-1 app-1-2-2-1=1pro/app-update

```
![[Pasted image 20250529114325.png]]
 -> 아직 업데이트가 끝나지 않아 트래픽이 아직 app-update쪽으로 가지 않는다 
 

#### (4-2) 기동되지 않는 App 업데이트
```yaml
spec:
      containers:
        - name: app-1-2-2-1
          image: 1pro/app-error   # 수정
```

> kubectl 명령으로 할 경우 

```bash
[root@k8s-master ~]# 
kubectl set image -n default deployment/app-1-2-2-1 app-1-2-2-1=1pro/app-error
```

새 이미지가 제대로 동작하는지 확인 > 계속 재시작하여 > 트래픽 옮김


## 3) 배포한 Object 삭제
```bash

[root@k8s-master ~]# 
kubectl delete -n default deploy app-1-2-2-1 

[root@k8s-master ~]# 
kubectl delete -n default svc app-1-2-2-1 

[root@k8s-master ~]# 
kubectl delete -n default hpa app-1-2-2-1
```



# 3. 서비스 안정화 및 인프라 환경 관리 코드화 

서비스 안정화
![[Pasted image 20250529114805.png]]

## 인프라 환경 관리 코드화 
1. 인프라에 대한 History 관리가 편해짐
2. 인프라 작업 추적 가능
3. 인프라 환경별 파일 생성
	1. 시간 있을 때 미리 구성 가능
	2. 작업은 Copy & Paste
4. 인프라 반복 작업 X, 퀄리티 향상에 집중
5. 새 인프라 작업시 이전 경험을 녹인 코드 활용



# 4. 쿠버네티스 엔지니어가 되려면
## 1) 전체 흐름 완성해보기
- 최소한의 도구 선택
	- 빌드 : gradle 
	- CI/CD : gitlab + Jenkins
	- 설치 : kubeadm
	- 모니터링 : Grafana, Prometheus
- 최소한의 기능 사용
![[Pasted image 20250529115209.png]]

## 2) 전문 분야 공부하기
- 배포 : argo, HELM, 컨테이너 레지스터리
![[Pasted image 20250529115335.png]]
- 클라우드
![[Pasted image 20250529115345.png]]
- 서비스 메시 
![[Pasted image 20250529115356.png]]


## 3) 상황별 공부하기 
프로젝트를 하는 상황일 때 (필요할 때 )
![[Pasted image 20250529115431.png]]




---
