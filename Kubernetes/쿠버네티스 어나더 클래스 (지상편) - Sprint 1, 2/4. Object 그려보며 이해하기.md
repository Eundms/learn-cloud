> 링크 : https://cafe.naver.com/kubeops/36


```shell
mkdir -p /root/k8s-local-volume/1231
```

### namespace 
- Object 그룹핑 역할

```yaml
apiVersion: v1 
kind: Namespace 
metadata: 
  name: anotherclass-123 
  labels: 
    part-of: k8s-anotherclass 
    managed-by: dashboard
```

### deployment 
- Pod 생성 & 업그레이드 
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: anotherclass-123
  name: api-tester-1231  # 한 네임스페이스내 이름 중복 x
  labels: # ✅ Deployment 자체 라벨  
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
    version: 1.0.0
    managed-by: dashboard
spec:
  selector:
    matchLabels: # ✅ 관리할 Pod을 식별하는 기준 -> ReplicaSet 적용 라벨
      part-of: k8s-anotherclass
      component: backend-server
      name: api-tester
      instance: api-tester-1231
  replicas: 2  # pod 2개
  strategy:
    type: RollingUpdate  # deploy 방식 
  template:
    metadata:
      labels: # ✅ ReplicaSet이 생성할 Pod에 적용되는 라벨  
        part-of: k8s-anotherclass
        component: backend-server
        name: api-tester 
        instance: api-tester-1231
        version: 1.0.0
    spec:
      nodeSelector: # node를 띄울 파드 선택 
        kubernetes.io/hostname: k8s-master
      containers:
        - name: api-tester-1231
          image: 1pro/api-tester:v1.0.0
          ports:
          - name: http
            containerPort: 8080
          envFrom: # 애플리케이션 환경변수 
            - configMapRef: # 값 제공 역할 
                name: api-tester-1231-properties # ✅ Config -> 컨테이너 연결 
          startupProbe: # 기동체크 (실패: 재기동)
            httpGet:
              path: "/startup"
              port: 8080
            periodSeconds: 5
            failureThreshold: 36
          readinessProbe: # 앱에 트래픽을 연결할지 결정하는 속성 (실패 : 연결해제)
            httpGet:
              path: "/readiness"
              port: 8080
            periodSeconds: 10
            failureThreshold: 3
          livenessProbe: # 서비스 관리 (실패: 재기동)
            httpGet:
              path: "/liveness"
              port: 8080
            periodSeconds: 10
            failureThreshold: 3
          resources: # Pod 하나당 자원을 결정 -> 설정 x : Pod가 노드의 자원을 모두...
            requests:
              memory: "100Mi"
              cpu: "100m"
            limits:
              memory: "200Mi"
              cpu: "200m"
          volumeMounts:
            - name: files  #(1)
              mountPath: /usr/src/myapp/files/dev # Pod 내부에 만들어지는 속성
            - name: secret-datasource #(2)
              mountPath: /usr/src/myapp/datasource
      volumes:
        - name: files  #(1)
          persistentVolumeClaim:
            claimName: api-tester-1231-files # ✅ PVC -> Pod 연결 
        - name: secret-datasource #(2)
          secret: 
            secretName: api-tester-1231-postgresql # ✅ Secret -> Pod 연결 
```


### service
- Pod에 트래픽 연결시켜주는 역할 

```yaml
apiVersion: v1
kind: Service
metadata:
  namespace: anotherclass-123
  name: api-tester-1231 # service와 deployment는 다른 object라 같은 네스 이름 같아도 됨
  labels: # Service 자체 라벨 (정보성)
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
    version: 1.0.0
    managed-by: dashboard
spec:
  selector: # ✅ Pod의 labels와 일치해야 함 -> 해당 Pod로 트래픽 전달 
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
  ports:
    - port: 80
      targetPort: http
      nodePort: 31231
  type: NodePort
```


### configmap, secret
```yaml
apiVersion: v1
kind: ConfigMap # pod에 환경 변수 값을 제공하는 역할 
metadata: # 환경변수로 들어갈 값 (1)
  namespace: anotherclass-123
  name: api-tester-1231-properties # ✅ ConfigMap 이름 : Deployment에서 연결할때 사용
  labels: # ✅ Configmap 자체 라벨 
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
    version: 1.0.0
    managed-by: dashboard
data: # (2)
  spring_profiles_active: "dev"
  application_role: "ALL"
  postgresql_filepath: "/usr/src/myapp/datasource/postgresql-info.yaml"
---
apiVersion: v1
kind: Secret # pod에 좀더 중요한 값 
metadata:
  namespace: anotherclass-123
  name: api-tester-1231-postgresql  # ✅ Secret 이름 : Deployment에서 연결할때 사용
  labels: # ✅ Secret 자체 라벨 
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
    version: 1.0.0
    managed-by: dashboard
stringData: # 아래 파일이 파드 안에 만들어짐 
  postgresql-info.yaml: |
    driver-class-name: "org.postgresql.Driver"
    url: "jdbc:postgresql://postgresql:5431"
    username: "dev"
    password: "dev123"
```


### pvc, pv
```yaml
apiVersion: v1
kind: PersistentVolumeClaim # pv 모드 지정 
metadata:
  namespace: anotherclass-123
  name: api-tester-1231-files # ✅ deployment에서 PVC 연결시 사용
  labels: # PVC 라벨 
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
    version: 1.0.0
    managed-by: kubectl
spec:
  resources:
    requests:
      storage: 2G # 사용할 저장 공간량 
  accessModes: # 접근 모드 
    - ReadWriteMany #읽기/쓰기 ㄱㄴ
  selector: # 
    matchLabels:
      part-of: k8s-anotherclass
      component: backend-server
      name: api-tester
      instance: api-tester-1231-files
---
apiVersion: v1
kind: PersistentVolume # 실제 볼륨 지정 -> Cluster 안에 포함되어 Cluster Level Object
metadata:
  name: api-tester-1231-files
  labels: # PV 라벨 
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231-files
    version: 1.0.0
    managed-by: dashboard
spec:
  capacity:
    storage: 2G
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  local: # path를 mount
    path: "/root/k8s-local-volume/1231"
  nodeAffinity: # node 지정 
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - {key: kubernetes.io/hostname, operator: In, values: [k8s-master]}
```

### HPA
- 부하에 따라 파드를 늘리고 줄이는 역할 
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  namespace: anotherclass-123
  name: api-tester-1231-default
  labels:
    part-of: k8s-anotherclass
    component: backend-server
    name: api-tester
    instance: api-tester-1231
    version: 1.0.0
    managed-by: dashboard
spec:
  scaleTargetRef: # 스케일링 대상
    apiVersion: apps/v1
    kind: Deployment # Deployment 지정
    name: api-tester-1231
  minReplicas: 2 # 최소
  maxReplicas: 4 # 최대 
  metrics: # cpu 평균 60 늘어나면 -> 스케일아웃!
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
  behavior: # pod 증가한 후 120초 동안 또 증가하지 않도록 설정 
    scaleUp:
      stabilizationWindowSeconds: 120
```



## 강의에서 배포한 Object 삭제 
```bash
[root@k8s-master ~]# 
kubectl delete ns anotherclass-123  # namespace 삭제 -> 안의 모든 object 삭제

[root@k8s-master ~]# 
kubectl delete pv api-tester-1231-files # pv는 namespace에 속하지 않음
```


# labels / selector /naming

### 오브젝트 간 연결 및 관리를 위해 labels, selector 가 사용된다 

label : object에 걸려있는 App정보를 바로 파악하기 위해 사용

part-of : 애플리케이션 전체 이름(kube-prometheus)
component : 각각의 기능들을 분리한 구성요소들
name : 어플리케이션 이름(들)  
instance : 목적에 따라 여러 개의 어플리케이션을 설치하기 위해 => `{{어플리케이션이름}-{식별자}}`
version : 앱 버전 변경시 수정 필요

### object에 대한 naming 
- namespace : monitoring => namespace에 담을 app의 범위 
- Statefulset :prometheus-k8s => `{instance}`
- Service => `{instance}`
- Configmap =>`{instance}-{목적}`

manged-by => 배포도구 (helm)



### Deployment -> ReplicaSet -> Pod와 PVC-> PV

> `Pod 업데이트 관리` -> 생성 -> `Pod 복제본 관리` -> 생성 -> `Pod`

Deployment : `{Deployment이름}` 을 보고   
Replicaset : `{Deployment이름}-{임의의식별자}` 가 
Pod : `{Deployment이름}-{임의의식별자}-{임의의식별자}`를 생성한다 

즉, ReplicaSet의 labels에 있는 내용이 Deployment의 selector에 있어야하고, 
Pod의 labels에 있는 내용이 Replicaset의 selector와 Service의 selector에 있어야 한다 